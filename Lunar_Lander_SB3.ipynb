{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBarCallbackWithFileLogging(BaseCallback):\n",
    "    def __init__(self, total_timesteps, log_file='training_log.csv', reward_log_file='episode_rewards_log.csv', verbose=0):\n",
    "        super(ProgressBarCallbackWithFileLogging, self).__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.pbar = None\n",
    "        self.episode_rewards = []  # Suivi des récompenses par épisode\n",
    "        self.max_reward = -np.inf  # Récompense maximale\n",
    "        self.current_episode_reward = 0  # Récompense cumulée pour l'épisode en cours\n",
    "        self.episode_count = 0  # Compteur d'épisodes\n",
    "        self.log_file = log_file   # Nom du fichier de log des statistiques globales\n",
    "        self.reward_log_file = reward_log_file  # Nom du fichier de log des récompenses par épisode\n",
    "\n",
    "        # Initialiser le fichier CSV pour les stats générales\n",
    "        with open(self.log_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Timestep', 'Mean Reward (1000 steps)', 'Max Reward', 'Actor Loss', 'Critic Loss'])\n",
    "\n",
    "        # Initialiser le fichier CSV pour les récompenses par épisode\n",
    "        with open(self.reward_log_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Episode', 'Reward'])\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        \"\"\"Initialisation de la barre de progression au début de l'entraînement.\"\"\"\n",
    "        print(\"Entraînement commencé...\")\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"Mise à jour de la barre de progression et écriture des infos dans les fichiers.\"\"\"\n",
    "        # Mise à jour de la barre de progression\n",
    "        self.pbar.update(1)\n",
    "\n",
    "        # Ajouter la récompense courante à l'épisode en cours\n",
    "        current_reward = self.locals['rewards'][0]\n",
    "        self.current_episode_reward += current_reward\n",
    "\n",
    "        # Mettre à jour la récompense maximale si nécessaire\n",
    "        if current_reward > self.max_reward:\n",
    "            self.max_reward = current_reward\n",
    "\n",
    "        # Si l'épisode se termine, enregistrer la récompense de l'épisode\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_count += 1\n",
    "\n",
    "            # Enregistrer la récompense cumulée de l'épisode dans le fichier CSV\n",
    "            with open(self.reward_log_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([self.episode_count, self.current_episode_reward])\n",
    "\n",
    "            # Réinitialiser la récompense pour le prochain épisode\n",
    "            self.current_episode_reward = 0\n",
    "\n",
    "        # Enregistrer des informations toutes les 1000 étapes\n",
    "        if self.num_timesteps % 1000 == 0:\n",
    "            mean_reward = np.mean(self.episode_rewards[-1000:]) if len(self.episode_rewards) >= 1000 else np.mean(self.episode_rewards)\n",
    "            actor_loss = self.locals.get('actor_loss', None)\n",
    "            critic_loss = self.locals.get('critic_loss', None)\n",
    "\n",
    "            # Écriture des statistiques globales dans le fichier CSV\n",
    "            with open(self.log_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([self.num_timesteps, mean_reward, self.max_reward, actor_loss, critic_loss])\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        \"\"\"Fermer la barre de progression et terminer le logging.\"\"\"\n",
    "        self.pbar.close()\n",
    "        print(f\"Entraînement terminé. Les informations sont enregistrées dans {self.log_file}\")\n",
    "        print(f\"Récompense maximale atteinte: {self.max_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres communs\n",
    "n_actions = 2\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Paramètres spécifiques TD3\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "buffer_size = int(2e5)\n",
    "tau = 0.05\n",
    "gamma = 0.98\n",
    "learning_starts = 10000\n",
    "n_steps = 100\n",
    "total_timesteps = n_steps * 1_000  # Réduire à 100k pour tester rapidement\n",
    "num_episodes_eval = 10\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[400, 300], qf=[400, 300])  # Architecture des réseaux d'acteur et de critique\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser les résultats\n",
    "seeds = [random.randint(0, 10000) for _ in range(10)]  # Générer 5 seeds aléatoires\n",
    "rewards_per_seed = {}  # Dictionnaire pour stocker les récompenses par seed\n",
    "\n",
    "# Configurer TensorBoard pour logger l'entraînement\n",
    "new_logger = configure(\"./logs/\", [\"tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle sur chaque seed\n",
    "for seed in seeds:\n",
    "    print(f\"Training with seed {seed}\")\n",
    "    \n",
    "    # Créer l'environnement avec la seed\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    # Initialiser le modèle TD3 avec des hyperparamètres personnalisés\n",
    "    model = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        action_noise=action_noise,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        buffer_size=buffer_size,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        train_freq=n_steps,\n",
    "        gradient_steps=-1,\n",
    "        learning_starts=learning_starts,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        device=\"cuda\"  # Utiliser le GPU si disponible\n",
    "    )\n",
    "\n",
    "    # Assigner le nouveau logger au modèle\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    # Créer et ajouter le callback de progression\n",
    "    progress_callback = ProgressBarCallbackWithFileLogging(total_timesteps, log_file=f\"training_log_seed_{seed}.csv\", reward_log_file=f\"episode_rewards_log_seed_{seed}.csv\")\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    model.learn(total_timesteps=total_timesteps, callback=progress_callback)\n",
    "\n",
    "    # Sauvegarder le modèle après l'entraînement pour chaque seed (optionnel)\n",
    "    model.save(f\"td3_lunar_lander_seed_{seed}\")\n",
    "\n",
    "    # Évaluation du modèle après l'entraînement\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes_eval):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    # Calculer et stocker la moyenne des récompenses pour cette seed\n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    rewards_per_seed[seed] = mean_reward\n",
    "    print(f\"Mean reward for seed {seed}: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les récompenses moyennes pour chaque seed\n",
    "print(\"\\nRewards per seed:\")\n",
    "for seed, reward in rewards_per_seed.items():\n",
    "    print(f\"Seed {seed}: Mean Reward = {reward}\")\n",
    "\n",
    "# Calculer la récompense moyenne sur les 10 seeds\n",
    "overall_mean_reward = np.mean(list(rewards_per_seed.values()))\n",
    "print(f\"\\nOverall mean reward across all seeds: {overall_mean_reward}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
